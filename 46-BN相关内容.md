BN的本质是防止梯度消失  
  
BN层的作用和优点：  
防止“梯度弥散”  
可以使用较大的学习率，加速网络收敛速度  
有正则作用  
  
训练时需要记录每个batch的均值方差，测试的时候需要计算期望，固定住  
BN计算，如果输出的blob大小为(N,C,H,W)，那么在每一层norm是对N*H*W个数值进行求均值以及方差的操作，即输出C个均值和C个方差  
BN还有两个需要学习的参数，尺度和偏移量，一是经过经过norm之后数据分布改变，有这两个参数可以有机会学回原始的分布，实现恒等变换，  
这样的目的是为了补偿网络的非线性表达能力，因为经过标准化之后，偏移量丢失。（我理解的，BN的提出是为了弥补sigmoid的梯度消失问题，归一化到梯度敏感的区域之后，虽然是有了梯度，但是那段函数偏向线性，多层线性的话没什么用，因此，加入这两个参数偏移以补偿非线性表达）。  
  
测试的时候均值和方差是怎么得来的，均值是训练的每个batch的均值的期望（求平均），方差是每个batch的方差的无偏估计（样本数改成n-1）。
不过实际上框架里的moving_mean和moving_var的更新就是一种近似，实现都是用的滑动平均。  
相关公式要会写写，比如怎么求  
还可以了解下IN，LN，GN等，效果不大  
BN是剩C，LN剩N，IN剩C和N，GN是剩N和一部分的C  
  
有些时候如果每个batch只能放一两个样本，BN的失效，可以用sync BN,（听说有时候GN也会有效果）  
每张卡每个batch都forward一次，每个batch记录均值方差，  
然后算出多卡的均值和方差，  
再用这个值去同步每张卡的梯度  
可参考https://www.cnblogs.com/makefile/p/batch-norm.html?utm_source=debugrun&utm_medium=referral
