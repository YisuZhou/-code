【混淆矩阵---确定截断点后，评价学习器性能】  
从混淆矩阵中可以获得很多信息  
TP（实际为正预测为正），FP（实际为负但预测为正），TN（实际为负预测为负），FN（实际为正但预测为负）  
查全率（召回率，recall）：样本中的正例有多少被预测准确了，衡量的是查全率，预测对的正例数占真正的正例数的比率：  
R＝ 预测出的并且是对的 / 所有本来应该预测出的 = TP / (TP+FN)  
查准率（精准率，Precision）：针对预测结果而言，预测为正的样本有多少是真正的正样本，衡量的是查准率，预测正确的正例数占预测为正例总量的比率：
P＝预测出的并且对的 / 所有预测出的 = TP / (TP+FP)  
准确率：反映分类器统对整个样本的判定能力，能将正的判定为正，负的判定为负的能力：  
Accuracy=(TP+TN) / (TP+FP+TN+FN)  
查准率和查全率通常是一对矛盾的度量，通常一个高，另外一个就低。两个指标都很重要，我们一般用两种方法综合考虑两个指标：  
（1）查准率=查全率的点，过了这个点，查全率将增加，查准率将降低。  
（2）F1 score，（1+beta^2）PR / beta^2 P+R  
  
【PR曲线】  
P-R曲线的P就是查准率（Precision），R就是查全率（Recall）。以P作为横坐标，R作为纵坐标，就可以画出P-R曲线。  
对于同一个模型，通过调整分类阈值，可以得到不同的P-R值，从而可以得到一条曲线（纵坐标为P，横坐标为R）。  
通常随着分类阈值从大到小变化（大于阈值认为P），Precision减小，Recall增加。  
比较两个分类器好坏时，显然是查得又准又全的比较好，也就是的PR曲线越往坐标（1，1）的位置靠近越好。  
若一个学习器的P-R曲线被另一个学习器完全”包住”，则后者的性能优于前者。  
当存在交叉时，可以计算曲线围住面积，不太容易判断，但是可以通过平衡点（查准率=查全率，Break-Even Point，BEP）来判断。  
  
【ROC曲线， AUC ----评价学习器性能，检验分类器对客户进行正确排序的能力】  
观察这个学习器利用所有可能的截断点（就是所有样本的预测结果）对样本进行分类时的效果，注意要先对所有可能的截断点进行排序，方便对比观察。  
ROC曲线描绘的是不同的截断点时，并以FPR和TPR为横纵坐标轴，描述随着截断点的变小，TPR随着FPR的变化。  
纵轴：TPR=正例分对的概率 = TP/(TP+FN)，其实就是Recall  
横轴：FPR=负例分错的概率 = FP/(FP+TN)，预测出来但是错的 / 所有负例  
ROC曲线越向上凸起，说明用了这个学习器在很小的代价（负例分错为正例，横轴）下达到了相对较大的查全率（TPR）。  
作图步骤：  
1. 根据学习器的预测结果（注意，是正例的概率值，非0/1变量）对样本进行排序（从大到小）-----这就是截断点依次选取的顺序  
2. 按顺序选取截断点，并计算TPR和FPR---也可以只选取n个截断点，分别在1/n，2/n，3/n等位置  
3. 连接所有的点（TPR，FPR）即为ROC图  
如果一个完全包住了另一个，那很明显，否则利用ROC曲线下的面积（AUC，area under ROC curve，是一个数值)进行比较  
  
【PR和ROC对比】  
与PR曲线相比，相对来讲ROC曲线会更稳定，在正负样本量都足够的情况下，ROC曲线足够反映模型的判断能力。  
而在正负样本分布得极不均匀(highly skewed datasets)的情况下（正样本极少），PRC比ROC能更有效地反映分类器对于整体分类情况的好坏。  
总之，只画一个曲线时，如果没有data imbalance,倾向于用ROC（更简洁，更好理解）。如果数据样本不均衡,分两种情况：  
情况1：如正样本远小于负样本，PR更敏感，因为用到了precision=(TP/(TP+FP))。  
情况2：正样本远大于负样本，PR和ROC差别不大，都不敏感。  
对于同一模型，PR和ROC曲线都可以说明一定的问题，而且二者有一定的相关性，如果想评测模型效果，也可以把两条曲线都画出来综合评价。  
具体到PR曲线和ROC曲线，他们的核心区别在TN。可以看出来，PR曲线其实不反应TN。  
所以，如果你的应用场景中，如果TN并不重要，那么PR曲线是一个很好的指标（事实上，Precision和Recall就是通过抹去TN，来去除极度的偏斜数据带来的影响，进而放大FP, FN和TP三者的关系的）。  
而ROC曲线则综合了TN, FP, FN和TP。虽然它对TN极度多的情况下，FP，FN和TP的变化不敏感。所以在TN没有那么多（数据没有那么偏斜），或者TN是一种很重要的需要考虑的情况下，ROC能反映出PR不能反映的问题。  
  
【AP计算方法】   
一种是VOC2007的11point方法，一种是VOC2010后的方法，最后是简单方法。VOC2007简单均值计算，VOC2012要精细一些，后者AP值高一些。  
详细和python的实现，见https://www.imooc.com/article/44040 和 https://www.cnblogs.com/makefile/p/metrics-mAP.html  
  
VOC2007年的方法：首先设定一组阈值，[0, 0.1, 0.2, …, 1]。然后对recall大于每一个阈值，也是右边最大P，都会得到一个对应的最大precision。  
这样，就计算出了11个precision。AP即为这11个precision的平均值。这种方法英文叫做11-point interpolated average precision。曲线下面积则为AP。  
  
VOC2010年之后的方法：新的计算方法假设这N个样本中有M个正例，那么会得到M个recall值（1/M, 2/M, ..., M/M）  
对于每个recall值r，可以计算出对应（r'>r）的最大pre（右边的最大P），然后对这M个precision值用recall的间隔加权平均得到最后的AP值。  
  
简单4方法：对于某类下全部的真实目标，将IOU>=0.5 的作为检测出来的目标，取不同的confidence 阈值计算对应的precision 和recall，  
对于每个recall，取其对应的最大precision，对这些precision 求平均即为该类的AP值。所有类的AP 值求平均即为mAP。  
  
在评测时，现阶段较为权威的MS COCO数据集上，对不同的IoU阈值（0.5-0.95，0.05为步长）分别计算AP，再综合平均，并且给出了不同大小物体分别的AP表现，对定位准确的模型给予奖励并全面地展现不同大小物体上检测算法的性能，更为科学合理。  
而在Pascal VOC中，检测结果只评测了IOU在0.5这个阈值下的AP值。  
