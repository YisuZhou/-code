以下从l1 l2分别作为 损失函数 和作为正则化 的时候来区别  
  
【L1范数损失函数】  
最小绝对值偏差（LAD），最小绝对值误差（LAE）。  
总的说来，它是把目标值与估计值的【绝对差值的总和】最小化  
  
相比较而言，L1对异常值比较鲁棒。  
L2是误差的平方，如果误差大于1，会差会大很多。  
因此L2损失函数的模型会对异常样本很敏感，需要调整整体来适应单个异常值，会牺牲其他小误差样本  
  
Smooth L1 Loss 修改L1 loss零点不平滑问题  
  
L1 Loss 的解是不稳定解。L2 得到的是稳定解。 
以下对稳定做一下解释：  
L1方法的不稳定性意味着，对于数据集的一个小的水平方向的波动，回归线也许会跳跃很大。  
在一些数据结构上，该方法有许多连续解；但是，对数据集的一个微小移动，就会跳过某个数据结构在一定区域内的许多连续解。  
在跳过这个区域内的解后，最小绝对值偏差线可能会比之前的线有更大的倾斜。  
相反地，最小平方法的解是稳定的，因为对于一个数据点的任何微小波动，回归线总是只会发生轻微移动；也就说，回归参数是数据集的连续函数。  
图见https://www.cnblogs.com/jclian91/p/9824310.html  
  
L1 loss 可能有多个解，L2 loss总是一个解。  
因为L2有解析解，但是l1没有<有种条条大路通罗马的感觉...只可意会>  
  
【L2范数损失函数】  
也被称为最小平方误差（LSE）。  
总的来说，它是把目标值与估计值的差值的【平方和】最小化  

平方化误差，loss可能会炸  
对离群点敏感  
L2 得到的是稳定解，总是惟一解。  
  
  
在机器学习中，正规化是防止过拟合的一种重要技巧。  
从数学上讲，它会增加一个正则项，防止系数拟合得过好以至于过拟合。  
L1与L2的区别只在于，L2是权重的平方和，而L1就是权重的和。  
【L1正则】  
在系数条件下效率高<可以用稀疏算法>，非稀疏情况下计算效率低<没有解析解>  
产生稀疏输出，天生就自带特征选择功能  
  
【L2正则】  
有解析解，一般情况下计算效率高  
非稀疏输出<会让权重不过大，产生靠近0的解>，没有特征选择功能  
