1 首先，选择不易梯度消失的激活函数(比如leaky relu)，像sigmoid会比较容易出现梯度消失，最大梯度为0.25  
2 然后，用BN，batchnorm就是通过对每一层的输出做scale和shift的方法，
通过规范化，把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布，
这样使得激活输入值落在非线性函数对输入比较敏感的区域，梯度变大，学习收敛速度快，加快训练速度。
3 再来，resnet结构，短路机制可以无损地传播梯度
4 还有些LSTM,分层预训练等，没用过就算了。
