最优化的方法无非都是解决两个问题  
（1）搜索方向的确定，更新的时候朝哪个方向变化  
（2）步长的确定，朝着搜索方向变化多少<这一步在深度学习里面一般不是根据目标函数求的，而是学习率来设定的>  

首先，梯度下降（最速下降）  
梯度向量，是函数变化增加最快的地方，方向导数的最大值。
对目标函数求偏导，每次更新按梯度减少的方向变化（当前位置的负梯度方向为搜索方向）  
但是梯度下降并不一定是全局下降最快的方向，它只是目标函数在当前点上下降最快的方向。  
最速下降法越接近目标值，步长越小，所以利用该方法需要多次迭代。  
  
  
然后，牛顿法（切线法） 
本质上就是用一个二次函数去近似原函数，然后用二次函数的极小值点作为下一次的迭代点。  
对单变量来说，对目标函数求导，计算当前点的切线斜率，找到这条切线与x轴的交点，将该点作为下一次的求导点。  
多变量来说，类似。需要用到一阶导数（雅可比矩阵）和二阶导数（Hessian阵）。  
但是Hessian阵需要正定才是凸函数，才收敛，而且需要计算Hessian阵的逆矩阵，计算量太大。  
  
于是，提出了拟牛顿法  
用来解决牛顿法本身的计算复杂、难以收敛、局部极小值问题  
拟牛顿法不需要计算二阶导数和逆矩阵，只需要一阶导数  
拟牛顿法和最速下降，都是只要求知道目标函数的梯度。  
拟牛顿法通过测量梯度的变化，构造目标函数在当前的二次模型，二次的部分不再用Hessian阵，而是用一个正定阵，这个正定阵是利用上一步的信息更新的。  
取这个二次模型的最优解作为搜索方向。  
  
具体更新步骤参考：https://zhuanlan.zhihu.com/p/37524275  
   
关于梯度下降和牛顿法的对比  
（1）梯度下降是一阶收敛，牛顿法是二阶收敛（看到了导数的变化率，看得更远），更快。但是都是局部算法。  
（2）梯度下降只考虑方向，牛顿法兼顾步长。  
（3）几何上说，牛顿法是二次曲面去拟合当前所处位置的局部曲面，应该能比梯度下降法的用一个平面去拟合来的好。  






