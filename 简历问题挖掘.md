Q: 情绪识别，积极中立消极，为什么要用AU，为什么不直接按照三分类任务去采集？为什么不直接用landmark的点来做？  
首先，这个任务曾经尝试过landmark特征点坐标，但是对应用场景仍然不鲁棒，这种方法多数是在考虑特征点之间的角度和距离，但是实际情况下，人物的头部上下左右运动角度的范围也比较大，这种情况下，一来landmark的准确性得不到保障，二来头部运动的不确定性导致很难用特征点之间的角度和距离去判定情绪（何况，特征点的个体差异还是比较大的）。但其实，AU在标注的阶段是参考了很多纹理状态的，这个也是坐标信息不包含的。  
另外，如果按三分类的任务去采集图像的话，一来可能类内距离会比较大，不利于学习，二来我们在做项目的时候还是希望能考虑数据的复用性，AU的方案还是比较灵活的。  
  
Q: label smoothing，作用，交叉熵反传  
onehot无法保证模型的泛化能力，容易造成过拟合;全概率和0概率鼓励所属类别和其他类别之间的差距尽可能加大，而由梯度有界可知，这种情况很难adapt。会造成模型过于相信预测的类别。  
分类任务训练过程中，网络会驱使自身往正确标签和错误标签差值大的方向学习，在训练数据不足以表征所以的样本特征的情况下，会导致网络过拟合。  
label smoothing的提出就是为了解决上述问题。最早是在Inception v2中被提出，是一种正则化的策略。其通过"软化"传统的one-hot类型标签，使得在计算损失值时能够有效抑制过拟合现象。如下图所示，label smoothing相当于减少真实样本标签的类别在计算损失函数时的权重，最终起到抑制过拟合的效果。（label smooth是把正确样本和其他类别的样本拉成一样，soft target是拉成不一样的。）  
  
Q: softmax-comp loss，细节。可能引申到hinge loss，SVM
一对图片（两张），先分别进行softmax（其实用sigmoid是一样的，只不过是为了框架的简洁性，只纳入softmax关键字），获得0-1之间的一个分值，s(丑)，s(美)，loss=max(0,s(丑)-s(美))  
形式上很像hinge loss，不过hinge loss有一个margin，不只是美的分数要高，而且要高出一个margin。  
SVM见另外的整理  
  
Q: 为什么年龄要用回归做，回归和分类做这个有什么区别，优缺点  
回归：主要优势是能够捕捉不同年龄之间的联系，由于年龄和人脸的很多随机因素相关，直接用回归可能会过拟合。  
分类：分类问题没有考虑到年龄的排序问题，而且有边界问题。  
另外一些，不知道对错  
回归可以考虑到预测和真实值之间的误差，比如10岁预测成30岁和预测成13岁是完全不一样的，但是普通的分类不能考虑到  
回归可能可以收敛得更快一点，因为梯度比较大，分类的梯度一直都小于1  
对于离群点，分类比回归更鲁棒。误差比较大的时候，对回归的影响比较大，但会分类影响很小  
L1比L2更鲁棒，分类比L1更鲁棒  
拓展：mixup  
  
Q: smooth l1，l1，l2，分类，区别，鲁棒性，优缺点等  
L2，对离群点不鲁棒，loss可能会炸  
smooth L1，在预测和真实差距不大的时候为L2 loss（我一般设置为x=1处为交界），一来可以解决离群点不鲁棒的问题，二来对预测的比较准确的样本惩罚可以小一点，学习地可以慢一点  
L1，对离群点比较鲁棒，与smooth l1不同的是没有l2的部分，对所有偏差一视同仁  
具体也可以参考另外的整理内容  
  
Q: 可能引申到l1和l2作为loss和正则项时候的区别
L1作为正则更容易获得稀疏解，L2作为正则更倾向学习比较小的值，而非0值  
理由可参考https://www.zhihu.com/question/37096933 第二个人的回答  
  
Q: l1_norm loss，为什么不直接使用分段加权loss，本质上是一样的
其实也是可以的，省的人为设置参数？  
  
Q: RPCA，解法，为什么不能用SGD,牛顿之类的  
传统凸优化的算法复杂度很高。  
  
Q：核范数
奇异值的和。核范数近似rank（似乎这个近似的的前提是奇异值不超过1），可以类似于L1近似L0的理解方式。  
  
Q：Inception系列


Q：Mobilenet系列


Q：卷积的参数量和FLOPS，包括两层3 * 3和一层 5 * 5，还有深度可分卷积等
参数量：  
一般卷积：  K * K * C1 * C2  
深度可分： K * K * C1 + 1 * 1 * C1 * C2  
计算量：  
（C1 * K * K + C1 * K * K - 1）* H * W * C2  
括号里第一项是乘法运算数，第二项是加法运算数，因为n个数相加，要加n-1次  
不考虑bias，会有一个-1，如果考虑bias，就没有 -1  
不考虑加法那直接第二项不要了  
  
两个3 * 3 和一个 5 * 5 感受野一样，但是参数量和计算量 5 * 5的更少  
  
Q：卷积的感受野计算，包括空洞卷积，pooling等  
从后往前计算，即，当前层的一个点，能反映前一层的多大区域  
output field size = ( input field size - kernel size + 2 × padding ) / stride + 1,  
变形之后得到input field size = （output field size - 1）× stride - 2 × padding + kernel size。  
因此可以通过后式从最后一层到第一次级联后计算在原图上的感受野。计算感受野时不需要考虑padding(因为我们不是在计算具体有边界的映射区域,而是计算抽象的大小):  
RF = 1 #待计算的feature map上的感受野大小  
for layer in （top layer To down layer）:  
    RF = ((RF -1)* stride) + kernel size  #关键
空洞卷积（dialated conv）的感受野的理论值（实际上将孔洞也算进感受区域了，如果计算非孔位置的面积那就和不带孔洞的感受野的值相等了）。  
卷积核的dialate属性定义为卷积核的元素间距，如 dialate=2 是每隔一个像素位置应用一个卷积元素，dialate=1就是普通的无孔卷积。  
那么大小fk，dialate=d的卷积核的等价卷积作用大小等价于kernel size为(fk−1)d+1 的卷积的感受野
  
Q：softmax交叉熵，sigmoid交叉熵，反传，结论和手推  
参考另外的整理内容  
  
Q：为什么回归和分类要用不同的Loss，回归用MSE而分类用交叉熵
参考另外的整理内容  
  
