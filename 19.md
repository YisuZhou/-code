Softmax  
![softmax公式]()

python实现  
import numpy as np  
def softmax(X):  
    X = X - np.max(X)  
    return np.exp(X) / np.sum(np.exp(X))   # 防止数值太大，溢出  
  

Q:为什么用softmax，而不直接用标准化将确定范围的值转换到区间[0,1]当中呢？  
A:如果计算结果里面有很大的值（正、负都有），通过normalization会有部分数会变得非常小。  
Softmax相较于normalization能够更好的区分不同类别的计算结果（结果区别比较小）  

  
交叉熵  
交叉熵刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。  
![交叉熵公式]()  
softmax交叉熵求导  
https://blog.csdn.net/bitcarmanlee/article/details/82320853  


选择softmax还是多个逻辑回归，看的是事件是否互斥。  
例如，在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器，还是使用 logistic 回归建立 k 个独立的二元分类器？   
这一选择取决于你的类别之间是否互斥  
例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种）  
此时你应该使用类别数 k = 4 的softmax回归。  
如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声。  
这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。   
